# File: .\code_extractor.py
import os

# Set the directory to your repository root
repo_dir = "."  # Change this to your repository folder if needed
output_file = "combined_python_code.txt"

with open(output_file, "w", encoding="utf-8") as outfile:
    for root, dirs, files in os.walk(repo_dir):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                outfile.write(f"# File: {file_path}\n")
                with open(file_path, "r", encoding="utf-8") as infile:
                    outfile.write(infile.read())
                outfile.write("\n\n")  # Add spacing between files

print(f"All Python files have been combined into '{output_file}'")


# File: .\setup.py
import setuptools
import subprocess
from pathlib import Path

# Try retrieving the version dynamically
try:
    version = (
        subprocess.check_output(["git", "describe", "--abbrev=0", "--tags"], stderr=subprocess.DEVNULL)
        .strip()
        .decode("utf-8")
    )
except subprocess.CalledProcessError:
    try:
        from importlib.metadata import version as get_version
        version = get_version("coord2region")
    except Exception:
        version = "0.0.1"  # Default fallback version

__version__ = version

# Read long description from README.md
long_description = Path("README.md").read_text(encoding="utf-8")

setuptools.setup(
    name="coord2region",
    version=__version__,
    author="Hamza Abdelhedi",
    author_email="hamza.abdelhedii@gmail.com",
    description="Find region name for a given MNI coordinate in a selected atlas",
    long_description=long_description,
    long_description_content_type="text/markdown",
    packages=setuptools.find_packages(),
    classifiers=[
        "Programming Language :: Python :: 3",
        "Operating System :: OS Independent",
    ],
    python_requires=">=3.8",
    install_requires=[
        "nibabel",  
    ],
    include_package_data=True,
)

# File: .\yorguin.py
def get_vertinfo(subjects_dir,parc='aparc.a2009s'):
    subject='fstemplate'
    src = mne.read_source_spaces(os.path.join(subjects_dir, subject, 'bem', subject + '-ico-4-src.fif'),verbose=False)
    verts = [v["vertno"] for v in src]
    #colormap = colormap

    subject = 'fsaverage' #source space subject ('fsaverage') did not match stc.subject (fstemplate)
    vmapping=get_vert_mappings(subject,subjects_dir,src=src,parc=parc)
    labmap=invert_vmapping(vmapping)
    mni = mne.vertex_to_mni(verts[:2], [0,1], subject, subjects_dir=subjects_dir)
    mni = np.concatenate(mni,axis=0)#.shape
    return {'mni':mni,'vmap':vmapping,'labmap':labmap}

def get_vert_mappings(subject,subjects_dir,parc='aparc',src=None,verbose=False):
    if src is None:
        src = mne.read_source_spaces(os.path.join(subjects_dir, subject, 'bem', subject + '-ico-4-src.fif'),verbose=False)

    labels_cortex = mne.read_labels_from_annot(
        subject, parc=parc, subjects_dir=subjects_dir,verbose=False)

    vertno = [s['vertno'] for s in src]
    nvert = [len(vn) for vn in vertno]
    if verbose:
        print(labels_cortex)

        print('the src space contains {} spaces and {} points'.format(
                len(src), sum(nvert)))
        print('the cortex contains {} spaces and {} points'.format(
                len(src[:2]), sum(nvert[:2])))
        print('the volumes contains {} spaces and {} points'.format(
                len(src[2:]), sum(nvert[2:])))

    labels_aseg = mne.get_volume_labels_from_src(src, subject, subjects_dir)

    label_vertidx_cortex = list()
    label_name_cortex = list()

    for label in labels_cortex:
        if label.hemi == 'lh':
            this_vertno = np.intersect1d(vertno[0], label.vertices)
            vertidx = np.searchsorted(vertno[0], this_vertno)
        elif label.hemi == 'rh':
            this_vertno = np.intersect1d(vertno[1], label.vertices)
            vertidx = nvert[0] + np.searchsorted(vertno[1], this_vertno)

        label_vertidx_cortex.append(vertidx)
        label_name_cortex.append(label.name)


    nv_ROIs_cortex = [len(lab) for lab in label_vertidx_cortex]
    n_ROIs_cortex = len(label_vertidx_cortex)

    # label_vertidx_deep contains the vertices of deep structures,
    # v=label_vertidx_deep[0] is the Left-Amygdala and so on
    label_vertidx_deep = list()
    label_name_deep = list()
    all_deep = list()

    n_hemi = 2
    for s, label in enumerate(labels_aseg):
        n_deep = s + n_hemi
        print(n_deep)
        print(label)
        this_vertno = np.intersect1d(vertno[n_deep], label.vertices)
        vertidx = sum(nvert[:n_deep]) + np.searchsorted(
                vertno[n_deep], this_vertno)

        label_vertidx_deep.append(vertidx)
        label_name_deep.append(label.name)

    n_ROIs_deep = len(label_vertidx_deep)

    # TEST
    if n_ROIs_deep>0:
        all_deep = np.concatenate(label_vertidx_deep)

        assert len(all_deep) == sum(nvert[2:]), 'Something wrong!!!'
        assert np.sum(all_deep - np.arange(sum(nvert[:2]), sum(nvert))) == 0, 'Something wrong!!!'  # noqa

    n_ROIs = n_ROIs_cortex + n_ROIs_deep

    cortex_dict = {'cortex-'+label_name_cortex[i]:label_vertidx_cortex[i] for i in range(len(label_name_cortex))}
    if n_ROIs_deep>0:
        deep_dict = {'deep-'+label_name_deep[i]:label_vertidx_deep[i] for i in range(len(label_name_deep))}
    else:
        deep_dict = {}
    all_dict = {**cortex_dict,**deep_dict}
    return all_dict

def invert_vmapping(vmapping):
    labmap ={}
    for lab,verts in vmapping.items():
        for v in verts:
            labmap[v]=lab
    return labmap

# As you may notice, there is a special treatment when deep sources are involved.
# 3:59
# I have a very similar function to get_vertinfo which is called get_ROI_info, I believe it was an older version, perhaps less general, I dont remember, probably better to compare with an llm:
def get_ROI_info(src, subject, subjects_dir, parc='aparc', aseg=False):

    labels_cortex = mne.read_labels_from_annot(
        subject, parc=parc, subjects_dir=subjects_dir)
    # print(labels_cortex)
    vertno = [s['vertno'] for s in src]
    nvert = [len(vn) for vn in vertno]
    n_vertices = sum(nvert)
    ROI_mapping = np.zeros(n_vertices, dtype=int)

    print('the src space contains {} spaces and {} points'.format(
            len(src), sum(nvert)))
    print('the cortex contains {} spaces and {} points'.format(
            len(src[:2]), sum(nvert[:2])))
    print('the volumes contains {} spaces and {} points'.format(
            len(src[2:]), sum(nvert[2:])))

    if aseg:
        labels_aseg = mne.get_volume_labels_from_src(
            src, subject, subjects_dir)

    label_vertidx_cortex = list()
    label_name_cortex = list()

    for label in labels_cortex:
        if label.hemi == 'lh':
            this_vertno = np.intersect1d(vertno[0], label.vertices)
            vertidx = np.searchsorted(vertno[0], this_vertno)
        elif label.hemi == 'rh':
            this_vertno = np.intersect1d(vertno[1], label.vertices)
            vertidx = nvert[0] + np.searchsorted(vertno[1], this_vertno)

        label_vertidx_cortex.append(vertidx)
        label_name_cortex.append(label.name)

    nv_ROIs_cortex = [len(lab) for lab in label_vertidx_cortex]
    n_ROIs_cortex = len(label_vertidx_cortex)

    cortex_dict = {
        label_name_cortex[i]:label_vertidx_cortex[i] for i in range(len(label_name_cortex))}  # noqa
    # label_vertidx_deep contains the vertices of deep structures,
    # v=label_vertidx_deep[0] is the Left-Amygdala and so on
    if aseg:
        label_vertidx_deep = list()
        label_name_deep = list()
        all_deep = list()

        n_hemi = 2
        for s, label in enumerate(labels_aseg):
            n_deep = s + n_hemi
            # print(n_deep)
            # print(label)
            this_vertno = np.intersect1d(vertno[n_deep], label.vertices)
            vertidx = sum(nvert[:n_deep]) + np.searchsorted(
                    vertno[n_deep], this_vertno)

            label_vertidx_deep.append(vertidx)
            label_name_deep.append(label.name)

        n_ROIs_deep = len(label_vertidx_deep)

        # TEST
        all_deep = np.concatenate(label_vertidx_deep)

        assert len(all_deep) == sum(nvert[2:]), 'Something wrong!!!'
        assert np.sum(all_deep - np.arange(sum(nvert[:2]), sum(nvert))) == 0, 'Something wrong!!!'  # noqa

        n_ROIs = n_ROIs_cortex + n_ROIs_deep

        deep_dict = {
            label_name_deep[i]:label_vertidx_deep[i] for i in range(len(label_name_deep))}  # noqa
        all_dict = {**cortex_dict, **deep_dict}
    else:
        n_ROIs = n_ROIs_cortex
        all_dict = {**cortex_dict}

    for nr, roi in enumerate(all_dict):
        # print(roi)
        idx = all_dict[roi]
        ROI_mapping[idx] = nr

    return all_dict, ROI_mapping

def get_morph(subject,subjects_dir,bidspath,subtemplate='fsaverage',spacing='ico-4',aseg=True,only_check=False):
    if aseg:
        asegstr = '-aseg'
    else:
        asegstr=''

    fpath = os.path.join(bidspath,f"sub-{subject}",f"sub-{subject}_modality-meg_type-epo-{spacing}{asegstr}-fwd.fif")

    if only_check:
        return os.path.exists(fpath)

    sbj = subject
    #fwd = mne.read_forward_solution(fpath)

    fsaverage_fpath = os.path.join(subjects_dir, f'{subtemplate}/bem/{subtemplate}-ico-4-src.fif')
    fsaverage_src = mne.read_source_spaces(fsaverage_fpath)

    vertices_to = [s['vertno'] for s in fsaverage_src]


    bem_dir = os.path.join(subjects_dir, sbj, 'bem')
    print('bem path {}'.format(bem_dir))
    fwd_fpath = fpath
    assert os.path.exists(fwd_fpath)
    print('source path {}'.format(fwd_fpath))

    fwd = mne.read_forward_solution(fwd_fpath)
    src = fwd['src']
    surf_src = mne.source_space.SourceSpaces(fwd['src'][:2])

    n_cortex = (src[0]['nuse'] + src[1]['nuse'])

    morph_surf = mne.compute_source_morph(
            src=surf_src, subject_from=sbj, subject_to=subtemplate,
            spacing=vertices_to, subjects_dir=subjects_dir)
    # M = morph_surf.morph_mat has dim N_fs x N_sbj  => M*data
    # gives the data in fs_average space
    print(morph_surf.kind)
    print(morph_surf.morph_mat.shape)
    print((src[0]['nuse'] + src[1]['nuse']))

    return morph_surf,n_cortex

# PALS_B12_Brodmann
# aparc.a2009s
# aparc
# aparc_sub
# Which were the freesurfer .annot files under "fsaverage\label"
# vertex to mni:
{"mni": {
        "0": [
            -36.806278228759766,
            -18.292722702026367,
            64.4615249633789
        ],
}# label to vertices:
{    "vmap": {
        "cortex-bankssts_1-lh": [
            129,
            290,
            1232,
            1233,
            1234,
            1236,
            1674,
            1675,
            1676,
            1677
        ]
}# vertices to label
{    "labmap": {
        "129": "cortex-bankssts_1-lh",
        "290": "cortex-bankssts_1-lh",
        "1232": "cortex-bankssts_1-lh",
        "1233": "cortex-bankssts_1-lh",
        "1234": "cortex-bankssts_1-lh",
}# vertices to tal coordinates
{    "tal": {
        "0": [
            -35.88577840194702,
            -24.24612915802002,
            59.44336203365325
        ],
}
# Finally I have this script to see if regions of differents parcellations intersect

import json
import numpy as np
aparc=json.load(open("vertinfo_aparc.json"))
pals = json.load(open("vertinfo_PALS_B12_Brodmann.json"))
aparc_sub=json.load(open("vertinfo_aparc_sub.json"))

regions = {}

for atlas_dict,atlas in zip([aparc, pals, aparc_sub], ['aparc', 'pals', 'aparc_sub']):
    regions[atlas] = {}
    for info in atlas_dict.keys():
        regions[atlas][info] = atlas_dict[info]


a = 'cortex-postcentral_1-rh' + '@' + 'aparc_sub'
b = 'cortex-Brodmann.43-rh' + '@' + 'pals'

region_a=a.split('@')[0]
region_b=b.split('@')[0]
atlas_a=a.split('@')[1]
atlas_b=b.split('@')[1]

vmap_a = regions[atlas_a]['vmap'][region_a]
vmap_b = regions[atlas_b]['vmap'][region_b]

intersect = set.intersection(set(vmap_a),set(vmap_b))


# get centroid of regions

coords_a = np.array([regions[atlas_a]['mni'][str(x)] for x in vmap_a])
coords_b = np.array([regions[atlas_b]['mni'][str(x)] for x in vmap_b])

centroid_a = np.mean(coords_a, axis=0)
centroid_b = np.mean(coords_b, axis=0)

dist_comp=np.abs(centroid_a-centroid_b)
dist = np.linalg.norm(centroid_a - centroid_b)


# File: .\coord2region\coord2region.py
import numpy as np
from typing import Any, Dict, List, Optional, Union, Tuple
from .fetching import AtlasFetcher

import numpy as np
from typing import Any, Dict, List, Optional, Union, Tuple


class VolumetricAtlasMapper:
    """
    Stores a single 3D volumetric atlas (a 3D numpy array + 4x4 affine) and provides
    coordinate <-> voxel <-> region lookups.

    Parameters
    ----------
    name : str
        Identifier for the atlas (e.g. "aal" or "brodmann").
    vol : np.ndarray
        A 3D array where each voxel stores an integer (region index).
    hdr : np.ndarray
        A 4x4 affine transform mapping voxel indices -> MNI/world coordinates.
    labels : dict or list or None
        Region labels. If a dict, keys should be strings for numeric indices, 
        and values are region names. If a list/array, it should match `index`.
    index : list or np.ndarray or None
        Region indices (numeric) corresponding to the labels list/array. Not needed if `labels` is a dict.
    system : str
        The anatomical coordinate space (e.g. "mni", "tal").

    Attributes
    ----------
    name : str
    vol : np.ndarray
    hdr : np.ndarray
    labels : dict or list or None
    index : list or np.ndarray or None
    system : str
    shape : tuple
        Shape of the volumetric atlas (vol.shape).
    """

    def __init__(self,
                 name: str,
                 vol: np.ndarray,
                 hdr: np.ndarray,
                 labels: Optional[Union[Dict[str, str], List[str], np.ndarray]] = None,
                 index: Optional[Union[List[int], np.ndarray]] = None,
                 system: str = 'mni') -> None:

        self.name = name
        self.vol = np.asarray(vol)
        self.hdr = np.asarray(hdr)
        self.labels = labels
        self.index = index
        self.system = system

        # Basic shape checks
        # if self.vol.ndim != 3:
        #     raise ValueError("`vol` must be a 3D numpy array.")
        if self.hdr.shape != (4, 4):
            raise ValueError("`hdr` must be a 4x4 transform matrix.")

        self.shape = self.vol.shape

        # If labels is a dict, prepare an inverse mapping:
        #   region_name -> region_index
        if isinstance(self.labels, dict):
            # Here we assume keys are index strings, values are region names
            self._label2index = {v: k for k, v in self.labels.items()}
        else:
            self._label2index = None

    # -------------------------------------------------------------------------
    # Internal lookups (private)
    # -------------------------------------------------------------------------
    def _lookup_region_name(self, value: Union[int, str]) -> str:
        """
        Return the region name corresponding to the given region index (int/str).
        Returns "Unknown" if not found.
        """
        if not isinstance(value, (int, str)):
            raise ValueError("value must be int or str")

        value_str = str(value)
        if isinstance(self.labels, dict):
            return self.labels.get(value_str, "Unknown")

        if self.index is not None and self.labels is not None:
            try:
                # If the index array is a list, we use index(); if np.ndarray, we do np.where
                if isinstance(self.index, list):
                    pos = self.index.index(int(value))
                else:
                    pos = int(np.where(self.index == int(value))[0][0])
                return self.labels[pos]
            except (ValueError, IndexError):
                return "Unknown"
        elif self.labels is not None:
            # labels might just be a list or array with no separate index
            try:
                return self.labels[int(value)]
            except (ValueError, IndexError):
                return "Unknown"

        return "Unknown"

    def _lookup_region_index(self, label: str) -> Union[int, str]:
        """
        Return the numeric region index corresponding to the given region name.
        Returns "Unknown" if not found.
        """
        if not isinstance(label, str):
            raise ValueError("label must be a string")

        if self._label2index is not None:
            return self._label2index.get(label, "Unknown")

        if self.index is not None and self.labels is not None:
            try:
                if isinstance(self.labels, list):
                    pos = self.labels.index(label)
                else:
                    pos = int(np.where(np.array(self.labels) == label)[0][0])
                # Return the corresponding numeric index from self.index
                if isinstance(self.index, list):
                    return self.index[pos]
                else:
                    return int(self.index[pos])
            except (ValueError, IndexError):
                return "Unknown"
        elif self.labels is not None:
            # If self.labels is just a list of strings
            try:
                return int(np.where(np.array(self.labels) == label)[0][0])
            except (ValueError, IndexError):
                return "Unknown"

        return "Unknown"

    # -------------------------------------------------------------------------
    # Region name / index
    # -------------------------------------------------------------------------
    def region_name_from_index(self, region_idx: Union[int, str]) -> str:
        """
        Public method: Return region name from numeric region index.
        """
        return self._lookup_region_name(region_idx)

    def region_index_from_name(self, region_name: str) -> Union[int, str]:
        """
        Public method: Return region index from region name.
        """
        return self._lookup_region_index(region_name)

    def list_all_regions(self) -> List[str]:
        """
        Return a list of all region names in this atlas.
        """
        if isinstance(self.labels, dict):
            return list(self.labels.values())
        elif self.labels is not None:
            return list(self.labels)
        else:
            return []

    def infer_hemisphere(self, region: Union[int, str]) -> Optional[str]:
        """
        Return the hemisphere ('L' or 'R') inferred from the region name,
        or None if not found or not applicable.
        """
        # Convert numeric region to string name, if needed:
        region_name = region if isinstance(region, str) else self._lookup_region_name(region)
        if region_name in (None, "Unknown"):
            return None
        
        if self.name == 'schaefer':
            hemi = region.split('_')[1]
            if hemi == 'LH':
                return 'L'
            if hemi == 'RH':
                return 'R'
            else:
                #TODO:logger.warn
                return None
        lower = region_name.lower()
        if lower.endswith('_l'):
            return 'L'
        elif lower.endswith('_r'):
            return 'R'
        else:
            return None

    # -------------------------------------------------------------------------
    # MNI <--> voxel conversions
    # -------------------------------------------------------------------------
    def mni_to_voxel(self, mni_coord: Union[List[float], np.ndarray]) -> Tuple[int, int, int]:
        """
        Convert an (x,y,z) MNI/world coordinate to voxel indices (i,j,k).
        Returns (i, j, k) as integers (rounded).
        """
        if not isinstance(mni_coord, (list, np.ndarray)):
            raise ValueError("`mni_coord` must be a list or numpy array.")
        pos_arr = np.asarray(mni_coord)
        if pos_arr.shape != (3,):
            raise ValueError("`mni_coord` must be a 3-element (x,y,z).")

        # MNI coordinates are usually in 3D (x, y, z), but to apply affine transformations, we need homogeneous coordinates (x, y, z, 1)
        homogeneous = np.append(pos_arr, 1)
        voxel = np.linalg.inv(self.hdr) @ homogeneous
        #self.hdr is a 4×4 affine transformation matrix that maps voxel indices ↔ MNI coordinates.
        #np.linalg.inv(self.hdr) computes the inverse of the affine matrix, which transforms MNI back to voxel space.
        #@ homogeneous applies the matrix multiplication.
        ijk = tuple(map(int, np.round(voxel[:3])))
        return ijk

    def voxel_to_mni(self, voxel_ijk: Union[List[int], np.ndarray]) -> np.ndarray:
        """
        Convert voxel indices (i,j,k) to MNI/world coordinates.
        Returns an array of shape (3,).
        """
        if not isinstance(voxel_ijk, (list, np.ndarray)):
            raise ValueError("`voxel_ijk` must be list or numpy array.")
        src_arr = np.atleast_2d(voxel_ijk)
        ones = np.ones((src_arr.shape[0], 1))
        homogeneous = np.hstack([src_arr, ones])
        transformed = homogeneous @ self.hdr.T
        coords = transformed[:, :3] / transformed[:, 3, np.newaxis]
        if src_arr.shape[0] == 1:
            return coords[0]
        return coords

    # -------------------------------------------------------------------------
    # MNI <--> region
    # -------------------------------------------------------------------------
    def mni_to_region_index(self, mni_coord: Union[List[float], np.ndarray]) -> Union[int, str]:
        """
        Return the region index for a given MNI coordinate.
        """
        ijk = self.mni_to_voxel(mni_coord)
        if any(i < 0 or i >= s for i, s in zip(ijk, self.shape)):
            return "Unknown"
        return int(self.vol[ijk])

    def mni_to_region_name(self, mni_coord: Union[List[float], np.ndarray]) -> str:
        """
        Return the region name for a given MNI coordinate.
        """
        region_idx = self.mni_to_region_index(mni_coord)
        if region_idx == "Unknown":
            return "Unknown"
        return self._lookup_region_name(region_idx)

    # -------------------------------------------------------------------------
    # region index/name <--> all voxel coords
    # -------------------------------------------------------------------------
    def region_index_to_mni(self, region_idx: Union[int, str]) -> np.ndarray:
        """
        Return an Nx3 array of MNI coords for all voxels matching the specified region index.
        Returns an empty array if none found.
        """
        # Make sure region_idx is an integer:
        try:
            idx_val = int(region_idx)
        except (ValueError, TypeError):
            return np.empty((0, 3))
        coords = np.argwhere(self.vol == idx_val)
        if coords.size == 0:
            return np.empty((0, 3))
        return self.voxel_to_mni(coords)

    def region_name_to_mni(self, region_name: str) -> np.ndarray:
        """
        Return an Nx3 array of MNI coords for all voxels matching the specified region name.
        Returns an empty array if none found.
        """
        region_idx = self.region_index_from_name(region_name)
        if region_idx == "Unknown":
            return np.empty((0, 3))
        return self.region_index_to_mni(region_idx)

class BatchAtlasMapper:
    """
    Provides batch (vectorized) conversions over many coordinates for a single VolumetricAtlasMapper.

    Example:
    --------
    mapper = VolumetricAtlasMapper(...)
    batch = BatchAtlasMapper(mapper)

    regions = batch.batch_mni_to_region_name([[0, 0, 0], [10, -20, 30]])
    """

    def __init__(self, mapper: VolumetricAtlasMapper) -> None:
        if not isinstance(mapper, VolumetricAtlasMapper):
            raise ValueError("mapper must be an instance of VolumetricAtlasMapper")
        self.mapper = mapper

    # ---- region name <-> index (batch) ---------------------------------------
    def batch_region_name_from_index(self, values: List[Union[int, str]]) -> List[str]:
        """
        For each region index in `values`, return the corresponding region name.
        """
        return [self.mapper.region_name_from_index(val) for val in values]

    def batch_region_index_from_name(self, labels: List[str]) -> List[Union[int, str]]:
        """
        For each region name in `labels`, return the corresponding region index.
        """
        return [self.mapper.region_index_from_name(label) for label in labels]

    # ---- MNI <-> voxel (batch) -----------------------------------------------
    def batch_mni_to_voxel(self, positions: Union[List[List[float]], np.ndarray]) -> List[tuple]:
        """
        Convert a batch of MNI coordinates to voxel indices (i,j,k).
        """
        positions_arr = np.atleast_2d(positions)
        return [self.mapper.mni_to_voxel(pos) for pos in positions_arr]

    def batch_voxel_to_mni(self, sources: Union[List[List[int]], np.ndarray]) -> np.ndarray:
        """
        Convert a batch of voxel indices (i,j,k) to MNI coords.
        Returns an Nx3 array.
        """
        sources_arr = np.atleast_2d(sources)
        return np.array([self.mapper.voxel_to_mni(s) for s in sources_arr])

    # ---- MNI -> region (batch) -----------------------------------------------
    def batch_mni_to_region_index(self, positions: Union[List[List[float]], np.ndarray]) -> List[Union[int, str]]:
        """
        For each MNI coordinate, return the corresponding region index.
        """
        positions_arr = np.atleast_2d(positions)
        return [self.mapper.mni_to_region_index(pos) for pos in positions_arr]

    def batch_mni_to_region_name(self, positions: Union[List[List[float]], np.ndarray]) -> List[str]:
        """
        For each MNI coordinate, return the corresponding region name.
        """
        positions_arr = np.atleast_2d(positions)
        return [self.mapper.mni_to_region_name(pos) for pos in positions_arr]

    # ---- region index/name -> MNI coords (batch) -----------------------------
    def batch_region_index_to_mni(self, indices: List[Union[int, str]]) -> List[np.ndarray]:
        """
        For each region index, return an array of MNI coords (Nx3) for that region.
        """
        return [self.mapper.region_index_to_mni(idx) for idx in indices]

    def batch_region_name_to_mni(self, regions: List[str]) -> List[np.ndarray]:
        """
        For each region name, return an array of MNI coords (Nx3) for that region.
        """
        return [self.mapper.region_name_to_mni(r) for r in regions]

from .fetching import AtlasFetcher  
class MultiAtlasMapper:
    """
    Manages multiple volumetric atlases by name, providing batch MNI->region or region->MNI queries
    across all atlases at once.

    Parameters
    ----------
    data_dir : str
        Directory for atlas data.
    atlases : dict
        Dictionary of {atlas_name: fetch_kwargs}, used by AtlasFetcher to retrieve each atlas.

    Attributes
    ----------
    mappers : dict
        Each entry {atlas_name: BatchAtlasMapper}
    """

    def __init__(self, data_dir: str, atlases: Dict[str, Dict[str, Any]]) -> None:
        self.mappers = {}

        atlas_fetcher = AtlasFetcher(data_dir=data_dir)
        for name, kwargs in atlases.items():
            atlas_data = atlas_fetcher.fetch_atlas(name, **kwargs)
            # Create a VolumetricAtlasMapper for this atlas_data
            vol = atlas_data["vol"]
            hdr = atlas_data["hdr"]
            labels = atlas_data.get("labels")
            index = atlas_data.get("index")
            # system = atlas_data.get("system", "mni")

            single_mapper = VolumetricAtlasMapper(
                name=name,
                vol=vol,
                hdr=hdr,
                labels=labels,
                index=index,
                system="mni"   # or read from atlas_data if you store that
            )
            batch_mapper = BatchAtlasMapper(single_mapper)
            self.mappers[name] = batch_mapper

    def batch_mni_to_region_names(self, coords: Union[List[List[float]], np.ndarray]) -> Dict[str, List[str]]:
        """
        Convert a batch of MNI coordinates to region names for ALL atlases.
        Returns a dict {atlas_name: [region_name, region_name, ...], ...}.
        """
        results = {}
        for atlas_name, mapper in self.mappers.items():
            results[atlas_name] = mapper.batch_mni_to_region_name(coords)
        return results

    def batch_region_name_to_mni(self, region_names: List[str]) -> Dict[str, List[np.ndarray]]:
        """
        Convert a list of region names to MNI coordinates for ALL atlases.
        Returns a dict {atlas_name: [np.array_of_coords_per_region, ...], ...}.
        """
        results = {}
        for atlas_name, mapper in self.mappers.items():
            results[atlas_name] = mapper.batch_region_name_to_mni(region_names)
        return results


# File: .\coord2region\coord2study.py
#!/usr/bin/env python3
import os
import sys
import logging
from typing import Any, Dict, List, Optional, Tuple, Union

from nimare.extract import fetch_neurosynth, fetch_neuroquery
from nimare.io import convert_neurosynth_to_dataset
from nimare.dataset import Dataset

# Setup logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
handler = logging.StreamHandler(sys.stderr)
formatter = logging.Formatter("%(levelname)s: %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)

try:
    from Bio import Entrez, Medline
    BIO_AVAILABLE = True
except ImportError:
    BIO_AVAILABLE = False
    logger.warning("Biopython not found. Abstract fetching will be disabled.")


def fetch_datasets(data_dir: str) -> Dict[str, Dataset]:
    """
    Fetch and convert Neurosynth and NeuroQuery datasets into NiMARE Dataset objects.
    
    :param data_dir: Directory to store downloaded data.
    :return: Dictionary of NiMARE Dataset objects indexed by dataset name.        
    """
    datasets: Dict[str, Dataset] = {}
    os.makedirs(data_dir, exist_ok=True)
    
    # Fetch Neurosynth data
    try:
        ns_files = fetch_neurosynth(
            data_dir=data_dir,
            version="7",
            source="abstract",
            vocab="terms",
            overwrite=False
        )
        ns_data = ns_files[0]  # fetch_neurosynth returns a list of dicts
        neurosynth_dset = convert_neurosynth_to_dataset(
            coordinates_file=ns_data["coordinates"],
            metadata_file=ns_data["metadata"],
            annotations_files=ns_data.get("features")
        )
        datasets["Neurosynth"] = neurosynth_dset
        logger.info("Neurosynth dataset loaded successfully.")
    except Exception as e:
        logger.warning(f"Failed to fetch/convert Neurosynth dataset: {e}")

    # Fetch NeuroQuery data
    try:
        nq_files = fetch_neuroquery(
            data_dir=data_dir,
            version="1",
            source="combined",
            vocab="neuroquery6308",
            type="tfidf",
            overwrite=False
        )
        nq_data = nq_files[0]
        neuroquery_dset = convert_neurosynth_to_dataset(
            coordinates_file=nq_data["coordinates"],
            metadata_file=nq_data["metadata"],
            annotations_files=nq_data.get("features")
        )
        datasets["NeuroQuery"] = neuroquery_dset
        logger.info("NeuroQuery dataset loaded successfully.")
    except Exception as e:
        logger.warning(f"Failed to fetch/convert NeuroQuery dataset: {e}")

    if not datasets:
        sys.exit("Error: No datasets could be loaded. Ensure you have internet access and NiMARE supports the datasets.")
    #TODO: Add more datasets as needed.
    return datasets


def _extract_study_metadata(dset: Dataset, sid: Any) -> Dict[str, Any]:
    """
    Given a study ID from a NiMARE Dataset, extract study metadata (title and abstract if available).
    
    :param dset: A NiMARE Dataset.
    :param sid: Study ID.
    :return: A dictionary with keys "id", "source", "title", and optionally "abstract".
    """
    study_entry: Dict[str, Any] = {"id": str(sid)}
    
    title: Optional[str] = None
    try:
        titles = dset.get_metadata(ids=[sid], field="title")
        if titles and titles[0] not in (None, "", "NaN"):
            title = titles[0]
    except Exception:
        try:
            authors = dset.get_metadata(ids=[sid], field="authors")
            year = dset.get_metadata(ids=[sid], field="year")
            if authors and year:
                title = f"{authors[0]} ({year[0]})"
        except Exception:
            title = None
    if title:
        study_entry["title"] = title

    # Optionally, retrieve abstract using PubMed via Entrez if email provided and Bio is available.
    if BIO_AVAILABLE and study_entry.get("id") and "email" in study_entry:
        pmid = str(sid).split("-")[0]  
        try:
            handle = Entrez.efetch(db="pubmed", id=pmid, rettype="medline", retmode="text")
            records = list(Medline.parse(handle))
            if records:
                rec = records[0]
                abstract_text = rec.get("AB")
                if abstract_text:
                    study_entry["abstract"] = abstract_text.strip()
                if "title" not in study_entry:
                    pub_title = rec.get("TI")
                    if pub_title:
                        study_entry["title"] = pub_title.strip()
        except Exception as e:
            logger.warning(f"Failed to fetch abstract for PMID {pmid}: {e}")
    return study_entry


def get_studies_for_coordinate(
    datasets: Dict[str, Dataset],
    coord: Union[List[float], Tuple[float, float, float]],
    email: Optional[str] = None
) -> List[Dict[str, Any]]:
    """
    Given an MNI coordinate and a dict of NiMARE datasets, return study metadata for studies
    that report that coordinate.
    
    :param datasets: Dictionary of NiMARE Dataset objects keyed by source name.
    :param coord: MNI coordinate [x, y, z].
    :param email: Email address to use with Entrez for abstract fetching (if available).
    :return: List of study metadata dictionaries.
    """
    # NiMARE expects a list of coordinates.
    coord_list = [list(coord)]
    studies_info: List[Dict[str, Any]] = []

    for source, dset in datasets.items():
        try:
            study_ids = dset.get_studies_by_coordinate(coord_list, r=0)
        except Exception as e:
            logger.warning(f"Failed to search coordinate {coord} in {source} dataset: {e}")
            continue
        if not study_ids:
            continue

        for sid in study_ids:
            study_entry = {"id": str(sid), "source": source}
            if email:
                Entrez.email = email
                study_entry["email"] = email  

            study_metadata = _extract_study_metadata(dset, sid)
            study_entry.update(study_metadata)
            studies_info.append(study_entry)

    return studies_info


# Example usage:
if __name__ == '__main__':
    DATA_DIR = "nimare_data"
    # Fetch datasets (Neurosynth and NeuroQuery)
    nimare_datasets = fetch_datasets(DATA_DIR)
    # Example coordinate (MNI)
    coordinate = [-30, -22, 50]
    # Optionally, provide an email for fetching PubMed abstracts.
    email_address = "babasanfour1503@gmail.com"  
    studies = get_studies_for_coordinate(nimare_datasets, coordinate, email=email_address)
    for study in studies:
        print(study)


# File: .\coord2region\fetching.py
import os
import logging
import numpy as np
from typing import Optional
from nibabel.nifti1 import Nifti1Image
  
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class AtlasFileHandler:
    """
    Handles file operations for atlas fetching.
    
    This class provides utility functions to:
      - Load local atlas files.
      - Download atlases from a URL.
      - Package files into a standardized dictionary with keys:
          'vol', 'hdr', 'labels', 'description', and 'file'.
    """
    def __init__(self, data_dir: Optional[str] = None):
        """
        :param data_dir: Directory to store/download atlas files.
             Defaults to a 'data' folder within a hidden '.coord2region' folder in the user's home directory.
        """
        # TODO check if the data_dir is a valid path
        # TODO check if data_dir is an absolute path before assigning home_dir
        home_dir = os.path.expanduser("~")
        if data_dir is None:
            self.data_dir = os.path.join(home_dir, 'coord2region_data')
        else:
            self.data_dir = os.path.join(home_dir, data_dir)
        os.makedirs(self.data_dir, exist_ok=True)
        self.nilearn_data = os.path.join(home_dir, 'nilearn_data')
        self.mne_data = os.path.join(home_dir, 'mne_data')

    def pack_vol_output(self, fname: str, desc: str = None):
        """
        Load an atlas file into a nibabel image (or numpy archive) and package it.
        
        :param fname: Path to the atlas file.
        :param desc: Short description.
        :return: A dictionary with keys: 'vol', 'hdr', 'labels', 'description', 'file'.
        :raises ValueError: If file format is unrecognized.
        """

        if isinstance(fname, str):
            path = os.path.abspath(fname)
            _, ext = os.path.splitext(fname)
            ext = ext.lower()

            if ext in ['.nii', '.gz', '.nii.gz']:
                # TODO add try-except block for loading the image
                import nibabel as nib
                img = nib.load(fname)
                vol_data = img.get_fdata(dtype=np.float32)
                hdr_matrix = img.affine
                return {
                    'vol': vol_data,
                    'hdr': hdr_matrix,
                }
 
            elif ext == '.npz':
                # TODO add try-except block for loading the archive
                arch = np.load(path, allow_pickle=True)
                vol_data = arch['vol']
                hdr_matrix = arch['hdr']
                # labels = None
                # if 'labels' in arch and 'index' in arch:
                #     labels = {idx: name for idx, name in zip(arch['index'], arch['labels'])}
                return {
                    'vol': vol_data,
                    'hdr': hdr_matrix,
                }
            else:
                raise ValueError(f"Unrecognized file format '{ext}' for path: {path}")
        else:
            if isinstance(fname,Nifti1Image):
                vol_data = fname.get_fdata(dtype=np.float32)
                hdr_matrix = fname.affine
                return {
                    'vol': vol_data,
                    'hdr': hdr_matrix,
                }

    def pack_surf_output(self, subject: str='fsaverage', subjects_dir: str=None, parc: str = 'aparc', **kwargs):
        """
        Load surface-based atlas using MNE from FreeSurfer annotation files.

        :param subject: The subject identifier (e.g., 'fsaverage').
        :param subjects_dir: Path to the FreeSurfer subjects directory.
        :param parc: The parcellation name (e.g., 'aparc', 'aparc.a2009s').
        :param kwargs: Additional keyword arguments.
        :return: A dictionary with keys: 'vmap', 'labmap', 'mni'.
        """
        import mne

        if not subjects_dir:
            subjects_dir = mne.datasets.sample.data_path() / "subjects"
            #mne.datasets.fetch_hcp_mmp_parcellation(subjects_dir=subjects_dir, verbose=True)

            #mne.datasets.fetch_aparc_sub_parcellation(subjects_dir=subjects_dir, verbose=True)

        labels = mne.read_labels_from_annot(
            subject, parc, subjects_dir=subjects_dir, **kwargs
        )

        
        labels
        lh_vert = src[0]['vertno']
        rh_vert = src[1]['vertno']
    
        cortex_dict = {
            label.name: (np.searchsorted(lh_vert, np.intersect1d(lh_vert, label.vertices))
                        if label.hemi == 'lh'
                        else len(lh_vert) + np.searchsorted(rh_vert, np.intersect1d(rh_vert, label.vertices)))
            for label in labels
        }

        labmap = {v: lab for lab, verts in cortex_dict.items() for v in np.atleast_1d(verts)}

        # Compute MNI coordinates for the cortical parts (assuming first two hemispheres)
        mni_list = mne.vertex_to_mni([lh_vert, rh_vert], [0, 1], subject, subjects_dir=subjects_dir)
        mni_coords = np.concatenate(mni_list, axis=0)
        # TODO improve consistency in the output format
        return {
            'vmap': cortex_dict,
            'labmap': labmap,
            'mni': mni_coords
        }

    def fetch_from_local(self, atlas_path: str):
        """
        Load an atlas from a local file.
        
        :param atlas_path: Path to the local atlas file.
        :return: The standardized atlas dictionary.
        """
        logger.info(f"Loading local atlas file: {atlas_path}")
        return self.pack_vol_output(atlas_path, desc="Local file")

    def fetch_from_url(self, atlas_url: str, **kwargs):
        """
        Download an atlas from a URL (if not already present) and load it.
        
        :param atlas_url: The URL of the atlas.
        :param kwargs: Additional parameters.
        :return: The standardized atlas dictionary.
        :raises RuntimeError: if the download fails.
        """
        # TODO document that the file name is expected to be in the URL
        import urllib.parse
        import requests
        #requests.packages.urllib3.disable_warnings()
        parsed = urllib.parse.urlparse(atlas_url)
        file_name = os.path.basename(parsed.path)
        if not file_name:
            file_name = "atlas_download.nii.gz"
        local_path = os.path.join(self.data_dir, file_name)

        if not os.path.exists(local_path):
            logger.info(f"Downloading atlas from {atlas_url}...")
            try:
                with requests.get(atlas_url, stream=True, timeout=30, verify=False) as r:
                    r.raise_for_status()
                    with open(local_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                logger.info(f"Atlas downloaded to {local_path}")
            except Exception as e:
                if os.path.exists(local_path):
                    os.remove(local_path)
                logger.exception(f"Failed to download from {atlas_url}")
                raise RuntimeError(f"Failed to download from {atlas_url}") from e
        else:
            logger.info(f"Atlas already exists: {local_path}. Skipping download.")

        return local_path


class AtlasFetcher:
    """
    Fetches neuroimaging atlases using various methods.
    
    This class uses an AtlasFileHandler instance for file operations and provides atlas-specific
    fetchers. Supported atlas identifiers include volumetric atlases such as:
      - "aal", "brodmann", "harvard-oxford", "juelich", "schaefer", "yeo", "aparc2009"
    
    In addition, this module now supports MNE-based, surface annotation atlases via:
      - "mne-annot" (generic annotation; requires keyword arguments 'subject' and 'subjects_dir')
      - "mne-aparc2009" (a convenience key that sets parc to 'aparc.a2009s')
      
    Each fetcher returns a standardized dictionary. For volumetric atlases the keys are:
      'vol', 'hdr', 'labels', 'description', 'file'.
    For MNE annotation atlases, additional keys include:
      'vmap' (label to vertex mapping), 'labmap' (vertex-to-label mapping), and 'mni' (MNI coordinates).
    """

    # Fallback URL for Talairach atlas .
    ATLAS_URLS = {
        'talairach': 'https://www.talairach.org/talairach.nii',
        'aal': 'http://www.gin.cnrs.fr/wp-content/uploads/AAL3v2_for_SPM12.tar.gz',
    }

    def __init__(self, data_dir: str = None):
        """
        :param data_dir: Directory to store/download atlas files.
        """

        self.file_handler = AtlasFileHandler(data_dir=data_dir)
        self.data_dir = self.file_handler.data_dir
        self.nilearn_data = self.file_handler.nilearn_data
        self.mne_data = self.file_handler.mne_data
        # self._atlas_fetchers = {
        #     #"aal": self._fetch_atlas_aal,
        #     "brodmann": self._fetch_atlas_brodmann,
        #     "harvard-oxford": self._fetch_atlas_harvard_oxford,
        #     "juelich": self._fetch_atlas_juelich,
        #     "schaefer": self._fetch_atlas_schaefer,
        #     "yeo": self._fetch_atlas_yeo,
        #     # MNE-based atlases:
        #     "aparc2009": self._fetch_atlas_aparc2009,
        # }

        from nilearn.datasets import fetch_atlas_aal, fetch_atlas_talairach, fetch_atlas_harvard_oxford, fetch_atlas_juelich, fetch_atlas_schaefer_2018, fetch_atlas_yeo_2011

        def _fetch_atlas_yeo_version(version='thick_17', **kwargs):
            from nilearn.datasets import fetch_atlas_yeo_2011
            fetched = self._fetch_atlas(fetch_atlas_yeo_2011, **kwargs)
            version = kwargs.get('version', 'thick_17')

            # Needs special care
            # thin/thick keys are the images
            # colors are the labels
            num = version.split('_')[-1]
            labels_file = fetched[f'colors_{num}']
            # read the labels file
            with open(labels_file, 'r') as f:
                lines = f.readlines()
            import re
            # replace any number of spaces with a single space in all lines
            lines = [re.sub(' +', ' ', line) for line in lines]
            labels = [line.strip().split(' ')[1] for idx, line in enumerate(lines)]
            output = {}
            output['labels'] = labels
            output['description'] = fetched['description']
            output['file'] = fetched[version]
            output['maps']=fetched[version] # this will be taken care of to make it an array later
            return output



        self._atlas_fetchers_nilearn = {
            'aal':  {'fetcher':fetch_atlas_aal,'default_kwargs': {'version': 'SPM12'}},
            'brodmann': {'fetcher':fetch_atlas_talairach,'default_kwargs': {'level_name': 'ba'}},
            'harvard-oxford': {'fetcher':fetch_atlas_harvard_oxford, 'default_kwargs': {'atlas_name': 'cort-maxprob-thr25-2mm'}},
            'juelich': {'fetcher':fetch_atlas_juelich, 'default_kwargs': {'atlas_name': 'maxprob-thr0-1mm'}},
            'schaefer': {'fetcher':fetch_atlas_schaefer_2018, 'default_kwargs': {}},
            'yeo': {'fetcher':_fetch_atlas_yeo_version, 'default_kwargs': {'version': 'thick_17'}},
        }

        self._atlas_fetchers_mne = {
            'aparc.a2009s': {'fetcher':self._fetch_atlas_aparc2009, 'default_kwargs':{'subject': 'fsaverage', 'subjects_dir': None, 'hemi': 'both'}},
        }


    # ---- Volumetric atlas fetchers using nilearn ----

    def _fetch_atlas(self, fetcher, **kwargs):
        try:
            return fetcher(data_dir=self.file_handler.data_dir, **kwargs)
        except Exception as e:
            logger.error(f"Failed to fetch atlas using primary data_dir: {self.file_handler.data_dir}", e, exc_info=True)
            logger.info(f"Attempting to fetch atlas using nilearn_data: {self.file_handler.nilearn_data}")
            return fetcher(data_dir=self.file_handler.nilearn_data, **kwargs)

    # ---- MNE-based (surface annotation) atlas fetcher ----
    
    def _fetch_atlas_aparc2009(self,**kwargs):
        return self.file_handler.pack_surf_output(**kwargs)
    
    # ---- Public method ----

    def fetch_atlas(self, atlas_name: str, atlas_url: str = None, version: str = None, **kwargs):
        """
        Fetch an atlas given an atlas identifier.
        
        The identifier can be:
            (a) A URL (starting with http:// or https://),
            (b) A local file path,
            (c) Nilearn or mne atlases atlases (e.g., "aal", "harvard-oxford", "aparc2009", "mne-annot", etc.).
        
        For MNE-based atlases (keys starting with "mne-"), additional keyword arguments are required:
            - subject: subject identifier (e.g., "fsaverage")
            - subjects_dir: path to the FreeSurfer subjects directory
        
        :param atlas_name: The atlas identifier or file path.
        :param version: Version specifier (used for certain atlases, e.g., AAL).
        :param atlas_url: (Optional) Override URL for fetching the atlas.
        :param kwargs: Additional keyword arguments for the specific fetcher.
        :return: A standardized atlas dictionary.
        :raises ValueError: if the atlas identifier is not recognized.
        """
        # Case (a): URL provided.
        if atlas_url is not None and (atlas_url.startswith('http://') or atlas_url.startswith('https://')):
            return self.file_handler.fetch_from_url(atlas_url, **kwargs)
        
        # Case (b): Local file path.
        atlas_file = kwargs.get("atlas_file")
        if atlas_file and os.path.isfile(atlas_file):
            return self.file_handler.fetch_from_local(atlas_file)
        elif os.path.isfile(os.path.join(self.data_dir, atlas_name)):
            return self.file_handler.fetch_from_local(os.path.join(self.data_dir, atlas_name))
    
        # Case (c): nilearn or mne atlases.
        key = atlas_name.lower()
        fetcher_nilearn = self._atlas_fetchers_nilearn.get(key, None)
        if fetcher_nilearn:
            try:
                this_kwargs = fetcher_nilearn['default_kwargs']
                this_kwargs.update(kwargs)
                if atlas_name != 'yeo':
                    fetched = self._fetch_atlas(fetcher_nilearn['fetcher'],**this_kwargs)
                else:
                    fetched = fetcher_nilearn['fetcher'](**this_kwargs)
                maphdr = self.file_handler.pack_vol_output(fetched["maps"])
                fetched.update(maphdr)
                fetched['vol']=np.squeeze(fetched['vol'])
                fetched['kwargs'] = this_kwargs
                if fetched.get('labels', None) is not None and isinstance(fetched['labels'], np.ndarray):
                    labels = fetched['labels'].tolist()
                    if isinstance(labels[0], bytes):
                        labels = [label.decode('utf-8') for label in labels]
                    fetched['labels'] = labels
                return fetched
            except Exception as e:
                logger.error(f"Failed to fetch atlas {key} using nilearn", e, exc_info=True)
                logger.warning(f"Attempting to fetch atlas {key} using url")
                if key in self.ATLAS_URLS:
                    return self.file_handler.fetch_from_url(self.ATLAS_URLS[key])
                else:
                    logger.error(f"Atlas {key} not found in available atlas urls")
        
        fetcher_mne = self._atlas_fetchers_mne.get(key, None)

        if fetcher_mne:
            try:
                this_kwargs = fetcher_mne['default_kwargs']
                this_kwargs.update(kwargs)
                fetched = fetcher_mne['fetcher'](parc=atlas_name,**this_kwargs)
                #maphdr = self.file_handler.pack_vol_output(fetched["maps"])
                #fetched.update(maphdr)
                #fetched['kwargs'] = this_kwargs
                return fetched
            except Exception as e:
                logger.error(f"Failed to fetch atlas {key} using mne", e, exc_info=True)
                return None
        raise ValueError(f"Unrecognized atlas name '{atlas_name}'. Available options: {list(self._atlas_fetchers.keys())}.")


# Example usage: # remove later
if __name__ == '__main__':
    af = AtlasFetcher(data_dir="atlas_data")
    # TODO: fix fetch using url!
    # atlas = af.fetch_atlas("aal", atlas_url="https://www.gin.cnrs.fr/wp-content/uploads/AAL3v2_for_SPM12.tar.gz")
    # logger.info(f"Fetched atlas: {atlas['description']} from file: {atlas['file']}")
    # atlas = af.fetch_atlas("talairach", atlas_url="https://www.talairach.org/talairach.nii")
    # logger.info(f"Fetched atlas: {atlas['description']} from file: {atlas['file']}")

    # TODO: test fetch from local file

    # TODO add "destrieux": self._fetch_atlas_destrieux, similar to mne-annot
    # TODO brodmann: self._fetch_atlas_brodmann is not downloading the file
    # TODO harvard-oxford: self._fetch_atlas_harvard_oxford fix labels fetching for this atlas
    # TODO juilich: self._fetch_atlas_juelich fix labels fetching for this atlas
    # TODO schaefer: self._fetch_atlas_schaefer check if labels are extracted correctly
    # TODO yeo: self._fetch_atlas_yeo check label extraction from description file
    # TODO add other nibabel, nilearn, mne atlases
    # atlas = af.fetch_atlas("yeo")
    # print(isinstance(atlas, dict))
    # print(atlas.keys())
    # print(atlas["labels"])

    # TODO: test fetching a surface-based atlas
    # atlas = af.fetch_atlas("mne-annot", subject="fsaverage", subjects_dir="mne_data")
    # print(isinstance(atlas, dict))

    # TODO: add save/load methods for created atlases
    # TODO: make output of fetch_atlas consistent
    # TODO: add method to list available atlases
    # TODO: refactor to use a single fetch method for all atlases
    # TODO: add method to fetch all atlases at once
    # TODO: check for atlases that supported by both mne and nilearn if else


# File: .\coord2region\__init__.py
from .coord2region import (
    VolumetricAtlasMapper,
    BatchAtlasMapper,
    MultiAtlasMapper
)
from .fetching import AtlasFetcher


# File: .\coord2region\utils\utils.py
import os



# File: .\coord2region\utils\__init__.py
from .utils import _fetch_labels


# File: .\tests\test_coord2region.py
import pytest
import numpy as np
from coord2region.fetching import AtlasFetcher
from coord2region.coord2region import VolumetricAtlasMapper, BatchAtlasMapper, MultiAtlasMapper

# Atlas Properties for Validation
PROPERTIES = {
    "harvard-oxford": {
        "infer_hemisphere": [('Frontal Pole', None)],
        "region2index": [('Insular Cortex', 2)],
        "allregions": 49,
    },
    "juelich": {
        "infer_hemisphere": [('GM Primary motor cortex BA4p', None)],
        "region2index": [('GM Amygdala_laterobasal group', 2)],
        "allregions": 63,
    },
    "schaefer": {
        "infer_hemisphere": [('7Networks_LH_Vis_1', 'L'), ('7Networks_RH_Default_PFCv_4', 'R')],
        "region2index": [('7Networks_LH_Vis_3', 2)],
        "allregions": 400,
    },
    "yeo": {
        "infer_hemisphere": [('17Networks_9', None)],
        "region2index": [('17Networks_2', 2)],
        "allregions": 18,
    }
}

# Test coordinates (ground truth needed)
TEST_MNIS = [[-54., 36., -4.],[10., 20., 30.]]
TEST_VOXELS = [[30, 40, 50]]


# Fixture: Load Fresh Atlas Data Per Test
@pytest.fixture(scope="function")
def fresh_atlas_data(request):
    """Loads and returns atlas data ('vol', 'hdr', 'labels') for a given atlas."""
    atlas_name = request.param
    print(f"\nLoading atlas: {atlas_name}")  # Debugging
    af = AtlasFetcher(data_dir="coord2region_data")
    return atlas_name, af.fetch_atlas(atlas_name)


# Fixture: Create Volumetric Mapper
@pytest.fixture(scope="function")
def volumetric_mapper(fresh_atlas_data):
    """Creates a fresh VolumetricAtlasMapper per test."""
    atlas_name, data = fresh_atlas_data
    return VolumetricAtlasMapper(
        name=atlas_name,
        vol=data["vol"],
        hdr=data["hdr"],
        labels=data.get("labels", None)
    )


# Fixture: Create BatchAtlasMapper for Generalized Atlas
@pytest.fixture(scope="function")
def vectorized_mapper(fresh_atlas_data):
    """Creates a BatchAtlasMapper for a given atlas."""
    atlas_name, data = fresh_atlas_data
    return BatchAtlasMapper(
        VolumetricAtlasMapper(
            name=atlas_name,
            vol=data["vol"],
            hdr=data["hdr"],
            labels=data.get("labels", None)
        )
    )


# Test: Debug Parameterization
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_debug_parametrize(fresh_atlas_data):
    atlas_name, _ = fresh_atlas_data
    print(f"\nRunning test for atlas: {atlas_name}")
    assert atlas_name in PROPERTIES.keys()


# Test: Atlas Structure
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_atlas_structure(fresh_atlas_data):
    atlas_name, data = fresh_atlas_data
    assert "vol" in data and data["vol"] is not None, f"{atlas_name} missing 'vol'"
    assert "hdr" in data and data["hdr"].shape == (4, 4), f"{atlas_name} missing 'hdr'"
    assert "labels" in data and len(data["labels"]) > 0, f"{atlas_name} missing 'labels'"


# Test: Hemisphere Inference
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_infer_hemisphere(volumetric_mapper, fresh_atlas_data):
    atlas_name, _ = fresh_atlas_data
    for region, expected in PROPERTIES[atlas_name]['infer_hemisphere']:
        result = volumetric_mapper.infer_hemisphere(region)
        assert result == expected, f"Error in infer_hemisphere for {atlas_name}: expected {expected}, got {result}"


# Test: Region Index Lookup
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_region_to_index(volumetric_mapper, fresh_atlas_data):
    atlas_name, _ = fresh_atlas_data
    for region, expected_index in PROPERTIES[atlas_name]['region2index']:
        idx = volumetric_mapper.region_index_from_name(region)
        assert idx == expected_index, f"Error in region2index for {atlas_name}: expected {expected_index}, got {idx}"


# Test: Batch MNI to Region Name
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_batch_mni_to_region_name(vectorized_mapper, volumetric_mapper):
    labels = volumetric_mapper.list_all_regions()[:5]
    coords_for_tests = [volumetric_mapper.region_name_to_mni(label)[0] for label in labels if volumetric_mapper.region_name_to_mni(label).shape[0] > 0]

    if not coords_for_tests:
        pytest.skip("No valid coords found for testing batch MNI->region")

    result = vectorized_mapper.batch_mni_to_region_name(coords_for_tests)
    assert len(result) == len(coords_for_tests)
    assert all(isinstance(r, str) for r in result)


# Test: Batch Region Index to Name
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_batch_region_name_from_index(vectorized_mapper):
    region_names = vectorized_mapper.batch_region_name_from_index([2, 3, 4])
    assert len(region_names) == 3


# Test: Batch Region Name to Index
@pytest.mark.parametrize("fresh_atlas_data", PROPERTIES.keys(), indirect=True)
def test_batch_region_index_from_name(vectorized_mapper):
    example_region1=PROPERTIES[vectorized_mapper.mapper.name]['region2index'][0][0]
    example_region2=PROPERTIES[vectorized_mapper.mapper.name]['infer_hemisphere'][0][0]
    region_indices = vectorized_mapper.batch_region_index_from_name([example_region1,example_region2, "Unknown Region"])
    assert len(region_indices) == 3


# Test: MultiAtlasMapper API
def test_multiatlas_api():
    """Test the high-level MultiAtlasMapper class."""
    c2r = MultiAtlasMapper(data_dir="coord2region_data", atlases={x: {} for x in PROPERTIES.keys()})
    coords = TEST_MNIS
    
    result_dict = c2r.batch_mni_to_region_names(coords)
    for atlas_name in PROPERTIES.keys():
        assert atlas_name in result_dict
        assert len(result_dict[atlas_name]) == len(coords)

    for region, _ in PROPERTIES[atlas_name]['region2index']:
        idx = c2r.batch_region_name_to_mni([region])

        for atlas2 in PROPERTIES.keys():
            if atlas2 == atlas_name:
                assert idx[atlas2][0].shape[0]!=0, f"Expected non-empty array for {atlas2} when querying {atlas_name} region"
            else:
                assert idx[atlas2][0].shape[0]==0, f"Expected empty array for {atlas2} when querying {atlas_name} region"

if __name__ == "__main__":
    # pytest.main([__file__])
    
    """
    to debug in vscode use this configuration in launch.json:
    {
        "name": "Pytest Debugger: Specific Test File",
        "type": "debugpy",
        "request": "launch",
        "module": "pytest",
        "args": [
            "tests/test_coord2region.py", // Specify the test file
            "-s",  // Show print statements
            "-v",  // Verbose output
            "--maxfail=1",  // Stop after first failure
            "--disable-warnings" // Suppress pytest warnings
        ],
        "console": "integratedTerminal",
        "justMyCode": false // Show full stack trace for debugging
    }
    """
    print('')

# File: .\tests\test_fetch.py
from coord2region import AtlasFetcher

"""
{
#"aal": self._fetch_atlas_aal,
"brodmann": self._fetch_atlas_brodmann,
"harvard-oxford": self._fetch_atlas_harvard_oxford,
"juelich": self._fetch_atlas_juelich,
"schaefer": self._fetch_atlas_schaefer,
"yeo": self._fetch_atlas_yeo,
# MNE-based atlases:
"aparc2009": self._fetch_atlas_aparc2009,
}

{
'vol': vol_data,
'hdr': hdr_matrix,
'labels': labels,
'description': desc,
'file': fname
}
"""
def test_fetch_nilearn_atlases():
    atlases = ["yeo","harvard-oxford","juelich", "schaefer"]#, "aparc2009"] #"brodmann", "aal",
    good_atlases = []
    bad_atlases = []
    for atlas in atlases:
        try:
            output=_fetch_atlas_helper(atlas)
            print(output)
            good_atlases.append(atlas)
            print('vol',output['vol'].shape)
            print('hdr',output['hdr'].shape)
            print('labels',len(output['labels']))

        except Exception as e:
            print(f"Error fetching atlas {atlas}: {e}")
            bad_atlases.append(atlas)
    print(f"Good atlases: {good_atlases}")
    print(f"Bad atlases: {bad_atlases}")
    assert len(bad_atlases) == 0, f"Failed to fetch atlases: {bad_atlases}"

def not_ready_test_fetch_mne_atlases():
    af = AtlasFetcher(data_dir="mne_data")
    atlas = af.fetch_atlas('aparc.a2009s')
    print(atlas)

def _fetch_atlas_helper(atlas_name):
    af = AtlasFetcher(data_dir="atlas_data")
    atlas = af.fetch_atlas(atlas_name)
    return atlas

if __name__ == "__main__":
    #not_ready_test_fetch_mne_atlases()
    test_fetch_nilearn_atlases()



# File: .\tests\__init__.py


